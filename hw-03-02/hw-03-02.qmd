---
title: "Homework 3- Classification: Alternate techniques"
author: "Riyanshi Bohra"
format: html
editor: visual
---

# Homework 3- Classification: Alternate Techniques

# Setting up

```{r}
# Installing and loading necessary packages

if(!require(pacman))
  install.packages("pacman")

pacman::p_load(
  C50,                
  caret,              
  e1071,               
  keras,              
  kernlab,            
  lattice,            
  MASS,               
  mlbench,            
  nnet,              
  palmerpenguins,     
  party,              
  partykit,           
  randomForest,       
  rpart,              
  RWeka,             
  scales,             
  tidymodels,         
  tidyverse,          
  xgboost            
)
```

```{r}
# To show fewer digits
options(digits=3)
```

# Importing the dataset

```{r}
# Loading the spam.csv dataset into the spam variable
spam_data <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv')
spam_data |> glimpse()
```

# Training and Test Data

```{r}

# Setting a seed for reproducibility
set.seed(123)  

# Splitting the spam_data dataset into 80% training and 20% testing subsets
inTrain <- createDataPartition(y = spam_data$yesno, p = .8)[[1]]
spam_data_train <- dplyr::slice(spam_data, inTrain)
spam_data_test <- dplyr::slice(spam_data, -inTrain)
```

# Fitting classification models to the Train Data

```{r}
#Creating 10-fold cross-validation indices for the training data
train_index <- createFolds(spam_data_train$yesno, k = 10)
```

## Decision Tree- Conditional Inference Tree

```{r}

# Training a Conditional Inference Tree model on the training data using cross-validation 
ctreeFit <- spam_data_train |> train(yesno ~ .,
  method = "ctree",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
ctreeFit

# Interpretation:
# The best model achieved an accuracy of 0.888 with a mincriterion of 0.01
```

```{r}
# Visualizing the structure of the trained Conditional Inference Tree model
plot(ctreeFit$finalModel)
```

## C 4.5 Decision Tree

```{r}
#Training a C4.5 decision tree (J48) on the training data using cross-validation

C45Fit <- spam_data_train |> train(yesno ~ .,
  method = "J48",
  data = _,
    tuneLength = 2,
    trControl = trainControl(method = "cv", indexOut = train_index))

```

```{r}
# Displaying the structure of the trained C4.5 decision tree
C45Fit$finalModel

# The tree makes decisions based on feature thresholds like "dollar <= 0.055".
```

## K-Nearest Neighbors

```{r}
# Training a k-Nearest Neighbors classifier on scaled training data using 10-fold cross-validation 
knnFit <- spam_data_train |> train(yesno ~ .,
  method = "knn",
  data = _,
  preProcess = "scale",
    tuneLength = 5,
  tuneGrid=data.frame(k = 1:10),
    trControl = trainControl(method = "cv", indexOut = train_index))
knnFit

# The best model used 1 neighbor and achieved an accuracy of 0.961.
```

```{r}
#Displaying the details of the trained k-NN model

knnFit$finalModel

# Interpretation
# The model used 1 neighbor and the training data had 2231 'n' labels and 1451 'y' labels.

```

## PART- Rule-based Classifier

```{r}
# Training a PART decision list (rule-based classifier) on the training data
rulesFit <- spam_data_train |> train(yesno ~ .,
  method = "PART",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", indexOut = train_index))
rulesFit
```

```{r}
# Displaying the rules generated by the trained PART decision list model

rulesFit$finalModel

# The model uses conditions like "dollar > 0.055" to make predictions.
```

## Linear Support Vector Machines

```{r}
# Training a linear Support Vector Machine on the training data
svmFit <- spam_data_train |> train(yesno ~.,
  method = "svmLinear",
  data = _,
    tuneLength = 2,
    trControl = trainControl(method = "cv", indexOut = train_index))
svmFit
```

```{r}
# Displaying details of the trained linear SVM model

svmFit$finalModel

# Interpretation:
# The model used 1516 support vectors and achieved a training error of 0.156437
```

## Random Forest

```{r}
# Train a random forest model on the spam data training set 

randomForestFit <- spam_data_train |> train(yesno ~ .,
  method = "rf",
  data = _,
    tuneLength = 2,
    trControl = trainControl(method = "cv", indexOut = train_index))
randomForestFit

#Interpretation:
# The trained random forest model on the spam data achieved an accuracy of 96.5% when considering 6 predictors at each split (`mtry = 6`)

```

```{r}
# Extract the final model details of the trained random forest.

randomForestFit$finalModel

# Interpretation:
# The trained random forest used 500 trees with 6 predictors tried at each split. The out-of-bag error estimate is 12.5%. The confusion matrix shows the model's performance on the training data

```

## Gradient Boosted Decision Trees (xgboost)

```{r}
# Train an eXtreme Gradient Boosting (XGBoost) model on the spam data training set with specified hyperparameters and cross-validation

xgboostFit <- spam_data_train |> train(yesno ~ .,
  method = "xgbTree",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", indexOut = train_index),
  tuneGrid = expand.grid(
    nrounds = 20,
    max_depth = 3,
    colsample_bytree = .6,
    eta = 0.1,
    gamma=0,
    min_child_weight = 1,
    subsample = .5
  ))
xgboostFit

# Interpretation: 
# The trained XGBoost model on the spam data achieved an accuracy of 87.3% with specified hyperparameters

```

```{r}
# Extract the final model details of the trained XGBoost model

xgboostFit$finalModel

# The trained XGBoost model used parameters such as a maximum depth of 3, learning rate (eta) of 0.1, and a colsample_bytree of 0.6

```

## Artificial Neural Network

```{r}
# Train a neural network model on the spam data training set
nnetFit <- spam_data_train |> train(yesno ~ .,
  method = "nnet",
  data = _,
    tuneLength = 3,
    trControl = trainControl(method = "cv", indexOut = train_index),
  trace = FALSE)
nnetFit

# Interpretation:
# The trained neural network model on the spam data achieved its best accuracy of 88% with a size of 5 neurons and a weight decay of 1e-04

```

```{r}
# Extract the final model details of the trained neural network

nnetFit$finalModel

# The trained neural network is a 6-5-1 network with 41 weights. It uses features like 'crl.tot', 'dollar', 'bang', 'money', 'n000', and 'make' for predictions

```

# Comparison of the Models

```{r}
# Collect resampling results from multiple models for comparison.

resamps <- resamples(list(
  ctree = ctreeFit,
  C45 = C45Fit,
  SVM = svmFit,
  KNN = knnFit,
  rules = rulesFit,
  randomForest = randomForestFit,
  xgboost = xgboostFit,
  NeuralNet = nnetFit
    ))
resamps

# Interpretation:
# Resampling results for 8 models (ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet) based on 10 resamples are collected

```

```{r}
# Calculating the summary statistics
summary(resamps)
```

```{r}
# Generate boxplots to visualize the performance distribution of multiple models across resamples

library(lattice)
bwplot(resamps, layout = c(3, 1))

# Interpretation:
# The boxplots visualize the distribution of model performance (e.g., accuracy) across multiple resamples for each model. randomForest and KNN perform significantly better compared to other models

```

```{r}
# Calculate pairwise differences between model performance metrics to determine if they are statistically significant

difs <- diff(resamps)
difs

```

```{r}
# Summarize the differences in model performance metrics
summary(difs)
```

# Applying Random forest to Test data

```{r}

# Predict the outcomes for the spam data test set using the trained random forest model

pr <- predict(randomForestFit, spam_data_test)
pr
```

# Comparing Decision Boundaries of Popular Classification Techniques

```{r}

# Creates a function to visualize decision boundaries for classification models.

library(scales)
library(tidyverse)
library(ggplot2)
library(caret)

decisionplot <- function(model, data, class_var, 
  predict_type = c("class", "prob"), resolution = 3 * 72) {
  # resolution is set to 72 dpi if the image is rendered  3 inches wide. 
  
  y <- data |> pull(class_var)
  x <- data |> dplyr::select(-all_of(class_var))
  
  # resubstitution accuracy
  prediction <- predict(model, x, type = predict_type[1])
  # LDA returns a list
  if(is.list(prediction)) prediction <- prediction$class
  prediction <- factor(prediction, levels = levels(y))
  
  cm <- confusionMatrix(data = prediction, 
                        reference = y)
  acc <- cm$overall["Accuracy"]
  
  # evaluate model on a grid
  r <- sapply(x[, 1:2], range, na.rm = TRUE)
  xs <- seq(r[1,1], r[2,1], length.out = resolution)
  ys <- seq(r[1,2], r[2,2], length.out = resolution)
  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))
  colnames(g) <- colnames(r)
  g <- as_tibble(g)
  
  ### guess how to get class labels from predict
  ### (unfortunately not very consistent between models)
  cl <- predict(model, g, type = predict_type[1])
  
  # LDA returns a list
  prob <- NULL
  if(is.list(cl)) { 
    prob <- cl$posterior
    cl <- cl$class
  } else
    if(!is.na(predict_type[2]))
      try(prob <- predict(model, g, type = predict_type[2]))
  
  # we visualize the difference in probability/score between the 
  # winning class and the second best class.
  # don't use probability if predict for the classifier does not support it.
  max_prob <- 1
  if(!is.null(prob))
    try({
      max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))
      max_prob <- max_prob[,1] - max_prob[,2]
    }, silent = TRUE) 
  
  cl <- factor(cl, levels = levels(y))
  
  g <- g |> add_column(prediction = cl, probability = max_prob)
  
  ggplot(g, mapping = aes(
    x = .data[[colnames(g)[1]]], y = .data[[colnames(g)[2]]])) +
    geom_raster(mapping = aes(fill = prediction, alpha = probability)) +
    geom_contour(mapping = aes(z = as.numeric(prediction)), 
      bins = length(levels(cl)), linewidth = .5, color = "black") +
    geom_point(data = data, mapping =  aes(
      x = .data[[colnames(data)[1]]], 
      y = .data[[colnames(data)[2]]],
      shape = .data[[class_var]]), alpha = .7) + 
    scale_alpha_continuous(range = c(0,1), limits = c(0,1), guide = "none") +  
    labs(subtitle = paste("Training accuracy:", round(acc, 2))) +
     theme_minimal(base_size = 14)
}
```

```{r}

# Extract and display a subset of 'spam_data' focusing on 'money', 'dollar', and 'yesno'.

set.seed(1000)
spam_data_edited <- as_tibble(spam_data) |>
  drop_na()

# Displays the first few rows of the 'spam_data_edited' dataset.
x <- spam_data_edited |> dplyr::select(money, dollar, yesno)
x
```

```{r}

# Generates a dataset 'x' and plots the distribution of 'money' vs. 'dollar' colored by 'yesno' class

set.seed(123)
n <- 100
x <- data.frame(
  money = rnorm(n),
  dollar = rnorm(n),
  yesno = as.factor(sample(c("Yes", "No"), n, replace = TRUE))
)

ggplot(x, aes(x = money, y = dollar, fill = yesno)) +  
  stat_density_2d(geom = "polygon", aes(alpha = after_stat(level))) +
  geom_point() +
  theme_minimal(base_size = 14) +
  labs(x = "money",
       y = "dollar",
       fill = "Yes/No",
       alpha = "Density")

# Interpretation: The plot visualizes the distribution of data points in the 'money' vs. 'dollar' plane. Points are differentiated by the 'yesno' category, and density contours help identify regions of high data concentration. The color gradient indicates the density level, with darker shades representing higher densities


```

```{r}
model <- x |> caret::knn3(yesno ~ ., data = _, k = 1)
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "kNN (1 neighbor)",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The kNN model with 1 neighbor shows sharp decision boundaries, potentially indicating overfitting(Training accuracy 1). The plot distinguishes between "Yes" and "No" based on the 'Money' and 'Dollar' features

```

```{r}
model <- x |> caret::knn3(yesno ~ ., data = _, k = 3)
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "kNN (3 neighbor)",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# With 3 neighbors in the kNN model, the decision boundaries appear smoother than with 1 neighbor, suggesting a more generalized model. Accuracy has decreased in comparison to the KNN with 1 neighbor

```

```{r}
model <- x |> caret::knn3(yesno ~ ., data = _, k = 9)
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "kNN (9 neighbor)",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The kNN model with 9 neighbors presents even smoother decision boundaries, indicating further generalization. The accuracy has further decreased to 0.67

```

# Naive Bayes Classifier

```{r}
model <- x |> e1071::naiveBayes(yesno ~ ., data = _)
decisionplot(model, x, class_var = "yesno", 
             predict_type = c("class", "raw")) + 
  labs(title = "Naive Bayes",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction") 

# Interpretation:
# The Naive Bayes model showcases more fluid decision boundaries, reflecting the probabilistic nature of this algorithm. The model uses 'Money' and 'Dollar' features for classifying "Yes" and "No". It has a training accuracy of 0.64

```

# Linear Discriminant Analysis

```{r}
model <- x |> MASS::lda(yesno ~ ., data = _)
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "LDA",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:

# The LDA model displays linear decision boundaries, as expected from a linear classifier. The classification is determined by the 'Money' and 'Dollar' features, distinguishing between "Yes" and "No" classes

```

# Multinomial Logistic Regression (implemented in nnet)

```{r}
# This code trains a multinomial logistic regression model using the nnet package
model <- x |> nnet::multinom(yesno ~., data = _)
```

```{r}
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "Multinomial Logistic Regression",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The plot shows how different regions of the "Money" vs "Dollar" space are classified. The colors represent predicted classes while the points are actual data
```

# Decision Trees

```{r}
# This code trains a Classification and Regression Tree (CART) model 
model <- x |> rpart::rpart(yesno ~ ., data = _)
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "CART",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The plot showcases the decision boundaries created by the CART model. It divides the space into regions based on the "Money" and "Dollar" features
```

```{r}
# This code trains a CART model with specific control parameters to make the tree grow more(overfitting)
model <- x |> rpart::rpart(yesno ~ ., data = _,
  control = rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "CART (overfitting)",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The decision boundaries are more intricate than the standard CART model, indicating a complex model that might be overfitting
```

```{r}
# This code trains a model using the C5.0 algorithm, an improved version of the C4.5 decision tree algorithm
model <- x |> C50::C5.0(yesno ~ ., data = _)
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "C5.0",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The decision boundaries for the C5.0 model are displayed. It showcases how the model classifies the space based on the features
```

# Random Forest

```{r}
model <- x |> randomForest::randomForest(yesno ~ ., data = _)
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "Random Forest",
       x = "Money",
       y = "Dollar",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The decision boundaries for the Random Forest model show the aggregated decision of multiple decision trees
```

# SVM

## Linear Kernel

```{r}
#  This code trains a Support Vector Machine (SVM) with a linear kernel
model <- x |> e1071::svm(yesno ~ ., data = _, kernel = "linear")
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "SVM (linear kernel)",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The decision boundaries for the linear SVM are shown. It uses straight lines to separate classes
```

## Radial Kernel

```{r}
# This code trains an SVM with a radial (Gaussian) kernel
model <- x |> e1071::svm(yesno ~ ., data = _, kernel = "radial")
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "SVM (radial kernel)",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The decision boundaries for the radial SVM are shown. Given the kernel, it can create more flexible boundaries compared to the linear SVM
```

## Polynomial kernel

```{r}
# This code trains an SVM with a polynomial kernel
model <- x |> e1071::svm(yesno ~ ., data = _, kernel = "polynomial")
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "SVM (polynomial kernel)",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The decision boundaries for the polynomial SVM are displayed. They can be curved based on polynomial transformations of the features
```

## Sigmoid Kernel

```{r}
# This code trains an SVM with a sigmoid kernel
model <- x |> e1071::svm(yesno ~ ., data = _, kernel = "sigmoid")
decisionplot(model, x, class_var = "yesno") + 
  labs(title = "SVM (sigmoid kernel)",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The decision boundaries for the sigmoid SVM are shown. The sigmoid kernel can also produce flexible boundaries, but its behavior is different from radial or polynomial kernels

```

# Single Layer Feed-forward Neural Networks

## 1 neuron

```{r}

# Train a neural network with 1 neuron in the hidden layer and visualize its decision boundary

model <- x |> nnet::nnet(yesno ~ ., data = _, size = 1, trace = FALSE)
decisionplot(model, x, class_var = "yesno", 
  predict_type = "class") + 
  labs(title = "NN (1 neuron)",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The neural network with 1 neuron has created a linear-like decision boundary to classify the data. Some misclassification can be observed

```

## 2 neurons

```{r}
# Train a neural network with 2 neurons in the hidden layer and visualize its decision boundary

model <-x |> nnet::nnet(yesno ~ ., data = _, size = 2, trace = FALSE)
decisionplot(model, x, class_var = "yesno", 
  predict_type = c("class")) + 
  labs(title = "NN (2 neurons)",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# With 2 neurons, the decision boundary becomes slightly more flexible and attempts to adapt to the data distribution. Still, some regions of misclassification are noticeable

```

## 4 neurons

```{r}
# Train a neural network with 4 neurons in the hidden layer and visualize its decision boundary

model <-x |> nnet::nnet(yesno ~ ., data = _, size = 4, trace = FALSE)
decisionplot(model, x, class_var = "yesno", 
  predict_type = c("class")) + 
  labs(title = "NN (4 neurons)",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# With 4 neurons, the decision boundary appears more complex and tries to capture intricate patterns in the data. This results in a better fit compared to the previous models

```

## 10 neurons

```{r}
# Train a neural network with 10 neurons in the hidden layer and visualize its decision boundary

model <-x |> nnet::nnet(yesno ~ ., data = _, size = 10, trace = FALSE)
decisionplot(model, x, class_var = "yesno", 
  predict_type = c("class")) + 
  labs(title = "NN (10 neurons)",
       shape = "Yes/No",
       fill = "Prediction")

# Interpretation:
# The neural network with 10 neurons has a highly flexible decision boundary, adapting closely to the data points. There's a risk of overfitting as it might be capturing noise too

```
